- wait queue implementation as cyclic buffer or in-kernel linked list
- spinlocks, general sheme as provided during classes
- atomic sleep_and_unlock
- everything is protected with spinlocks, so safety is trivial
- spinlocks must reside in userspace, semaphore value also
- we can put process to list associated with futex sleep queue before
invoking sleep_and_unlock as we're protected by spinlock, so all those
operations are seen atomic to others

problems:
- how to unlock spinlock from ipc server address space?
	* cant be done in kernel, because sheduler won't pick process holding
	spinlock as it was put to sleep
	* we must tell sleep_and_unlock which address space and address itself
	is connected with this futex

comparsions:
(*)
- in-kernel (holding by IPC) wait queue: logical approach, more scalable,
initializing and destroing futex will issue syscall (but that's fine)
- user space: we can do more logic in user-space (which is not very helpful,
because everyone is waiting for us to set up everything), less syscalls,
less code (pick this one first)

backend api:
* futex_sleep(futex_t*)
* futex_wake(futex_t*)

kernel api:
* sleep_and_unlock(spinlock_t*) - sets rts flag (which?) of current process
and unlocks spinlock in this process address space
* wakeup(pid_t) - clears rts flag (same as above) of process with given pid
